# 2023.2.24 第一次实验
## 对模型的修改：
    将视频序列的长度由15增加到了20，将模型隐神经元的数目增加到720， 初始学习率增加到0.01，epoch数增加到100
## 结果：
    最终的loss由570降低至460，但是输出的关节姿态仍然是混乱的
## 分析：
    模型拟合能力不够，并且将头部的方向向量和关节位置向量拼接在一起共同学习可能不是一个好主意

# 2023.2.26 第二次实验
## 对模型的修改
    将lr_steps由[20, 40]修改为[50, 100],epoch数目增加到150,将模型的head数目增加到10个,transformer子网络数目由6个增加到8个
## 结果：
    loss降低为411
## 分析：
    模型的拟合能力还是不够 需要将头部向量和姿态向量分开进行学习

# 2023.2.26 第三次实验
## 对模型的修改:
    将序列长度由20增加到了30，其余保持不变；修改了模型推理测试的代码
## 结果：
    loss提高到650
## 分析：
    模型没有拟合，学习能力不够

# 2023.2.26 第四次实验
## 对模型的修改:
    将模型的深度由N=8增加到N=16，梯度裁剪限制由30上升到50
## 结果：
    loss 697
## 分析：
    模型深度增加，但是loss上升？

# 2023.2.27 第五次实验
## 对模型的修改:
    将头部向量和姿态向量分开进行学习
    将模型的序列长度由30下降为20，增加随机数种子；
    更换优化器：将SGD替换为Adam + scheduler
    调整学习率，将初始lr 调整为0.1
## 结果：
    loss下降到20多，主要是因为去除了头部方向向量，loss的计算方式改变了
    测试出来的位姿还是很糟糕
## 分析：
    可能是因为在解码的时候没有一个正确的示范位姿引导，考虑在训练时加入示范位姿
    
# 2023.2.28 第六次实验
## 对模型的修改:
    将模型的dropout由0.5调整到0.1
    取消了positional encoding
## 结果：
    loss下降到10 但是输出的骨架信息还是毫无意义的混乱
## 分析：
    暂不清楚是dropout的问题还是取消位置编码的问题
    另外，这种直接输入特征来进行学习的transformer方法是否真的有效？
    模型学习到的似乎只是特征在未来的分布，而不是关节位置信息；
    下一步：不直接输入特征，而是输入图像来提取学习特征

# 2023.2.28 第七次实验
## 对模型的修改:
    改变模型的初始化方式：由xavier初始化更改为全0初始化
## 结果：
    loss下降到10 没什么用


# 2023.2.28 第八次实验
## 对模型的修改:
    使用完整的数据集，提高了数据量
## 结果：
    loss下降到8左右，结果还是不能令人满意，
## 分析：
    1. 师兄提供的一个想法：每一个sublayer可以额外做一个映射将120维向量映射到45维向量，这样不需要再输出之后重新做一个linear（这样可能会造成偏差），并且还可以增加模型的学习能力
    2. 之前为了确保每一个位姿坐标都在(-1, 1)之间，我们会在输出上强制加一个tanh函数，仔细想想：label位姿使用((x-x_min)/(x_max-x_min)-0.5) / 0.5计算得到的，而不是用tanh，这样会造成训练输出和label之间的数据分布不一样
    
# 2023.3.1 第九次实验
## 对模型的修改:
    仍然使用subject_01这一小部分数据集
    不再使用tanh这一激活函数，而是在genarator中使用((x-x_min)/(x_max-x_min)-0.5)/0.5这样的输出位姿
## 结果：
    虽然loss的大小还是10左右，但是输出的位姿明显有了规律 不再是一团乱麻
## 分析：
    下一步考虑改变一下坐标的分布：比如把x坐标全部拼接在一起，然后把y坐标全部拼接在一起...

# 2023.3.1 第十次实验
## 对模型的修改:
    将标签的组织形式从(x,y,z....x,y,z)变换成(x,x,x,..y,y,y...z,z,z)
    重新加入位置编码，但是将原特征乘上了10，并且给位置编码乘上了0.1，这样做是因为我们人工提取的特征都比较小，如果再加上很大的位置编码的话，模型就只能学习位置编码之间的关系了

    另外，我发现loss在计算时，关节的位置全部搞错了！
    调整之后重新计算
    重新跑实验九
## 结果
## 分析

# 2023.3.9 第十八次实验
## 对模型的修改:
    使用了bvh-converter这个库来解析bvh文件，并且得到了关节的3D位置坐标,使用正确的位姿进行训练 
    另外，实验发现在训练时，解码时使用的第一个标签是ground-truth，z这应该是不正确的，因为在推理时没有ground-truth信息，应该用一个通用的普通位姿作为解码开始的信号，比如关节角度全为0的位姿？这个明天问一下师兄。
## 结果：
    loss下降到20左右，输出的人体位姿开始有意义，但是transformer输出的整体的动作序列还是不太会变化；左转和右转这些动作的位姿模型并没有学习到

# 2023.3.13 第十九次实验
## 对模型的修改：
    使用我们自己的EgoMotion数据集进行训练和测试
## 结果：
    


# 2023.3.13 第二十次实验
## 对模型的修改：
    加入positional embedding
## 结果：
    loss为22.79